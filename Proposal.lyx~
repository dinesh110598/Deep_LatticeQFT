#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Autoregressive networks based on 
\begin_inset Formula $d-1$
\end_inset

 dimensional convolutions for lattice field theory simulations
\end_layout

\begin_layout Section
The autoregressive relation and scalar lattice field theory
\end_layout

\begin_layout Standard
The systems of interest consist of a box lattice of length 
\begin_inset Formula $L$
\end_inset

 in 
\begin_inset Formula $d$
\end_inset

 dimensions where every position is labeled using a 
\begin_inset Formula $d$
\end_inset

-dimensional vector 
\begin_inset Formula $\bm{x}\in[1,L]^{d}$
\end_inset

.
 The system state/configuration is described using scalar values at every
 position 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

.
 The configurations obey the Boltzmann distribution:
\begin_inset Formula 
\begin{equation}
p\left(\{\phi(\bm{x})\}_{\bm{x}\in[1,L]^{d}}\right)=e^{-S[\phi]}/Z\label{eq:boltzmann}
\end{equation}

\end_inset

where the 
\emph on
action
\emph default
 
\begin_inset Formula $S[\phi]$
\end_inset

 is a functional of the field values 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

.
 The d-dimensional positions 
\begin_inset Formula $\bm{x}$
\end_inset

 maybe replaced with a 1 dimensional ordering:
\begin_inset Formula 
\begin{equation}
k=\left({\displaystyle \sum_{i=1}^{d}}(x_{i}-1)L^{i-1}\right)+1\label{eq:ordering}
\end{equation}

\end_inset

where 
\begin_inset Formula $x_{i}$
\end_inset

 are the components of 
\begin_inset Formula $\bm{x}$
\end_inset

 and 
\begin_inset Formula $k\in[1,N=L^{d}]$
\end_inset

.
 Based on this ordering, we can write down the probability distribution
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:boltzmann"

\end_inset

 as a product of conditional distirbutions at every position:
\begin_inset Formula 
\begin{align}
p(\{\phi_{k}\}) & =p(\phi_{1},\phi_{2}\dots\phi_{N})=p(\phi_{1})p(\phi_{2}|\phi_{1})\dots p(\phi_{N}|\phi_{N-1}\dots\phi_{2},\phi_{1})\nonumber \\
 & =\prod_{k\in[1,N]}p(\phi_{k}|\phi_{<k})\label{eq:chainrule}
\end{align}

\end_inset

This is the chain rule of conditional probabilities based on Bayes theorem
 or 
\emph on
autoregressive relation
\emph default
.
 This mathematical relation is the basis of image and audio generation algorithm
s in deep learning such as MADE
\begin_inset CommandInset citation
LatexCommand cite
key "made"
literal "false"

\end_inset

 and PixelCNN
\begin_inset CommandInset citation
LatexCommand cite
key "pixelcnn"
literal "false"

\end_inset

.
 Our system of interest is the scalar lattice field theory whose action
 is given by:
\begin_inset Formula 
\begin{equation}
S[\phi]={\displaystyle \sum_{\bm{x}\in[1,L]^{d}}}\left[\phi(\bm{x}){\displaystyle \sum_{\bm{y}}}\boxempty(\bm{x},\bm{y})\phi(\bm{y})+m^{2}\phi(\bm{x})^{2}+\lambda\phi(\bm{x})^{4}\right]\label{eq:latticeFT-action}
\end{equation}

\end_inset

where 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $m$
\end_inset

, 
\begin_inset Formula $\lambda$
\end_inset

 are the lattice spacing, mass and coupling respectively.
 Assuming open boundary conditions, we can expand the d'Alembertian term
 in the RHS as:
\begin_inset Formula 
\[
{\displaystyle \sum_{\bm{x}\in[1,L]^{d}}}\phi(\bm{x}){\displaystyle \sum_{\bm{y}}}\boxempty(\bm{x},\bm{y})\phi(\bm{y})=\sum_{\mu=1}^{d}\sum_{x_{\nu=\mu}\in[2,L-1],x_{\nu}\neq[1,L]}2\phi(\bm{x})^{2}-\phi(\bm{x})\phi(\bm{x}-\hat{\mu})-\phi(\bm{x})\phi(\bm{x}+\hat{\mu})
\]

\end_inset

and 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

 can take any real value.
 Note that the action 
\begin_inset Formula $S$
\end_inset

 depends only on nearest neighbour product/interaction terms like 
\begin_inset Formula $\phi(\bm{x})\phi(\bm{x}-\hat{\mu})$
\end_inset

 besides powers of 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

 alone.
\end_layout

\begin_layout Section
Smaller dependency sets of conditional distributions due to nearest neighbour
 interactions
\end_layout

\begin_layout Standard
Examining the 
\begin_inset Formula $k$
\end_inset

th conditional probability 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:chainrule"

\end_inset

, its distribution in general depends on 
\begin_inset Formula $k-1$
\end_inset

 values in 
\begin_inset Formula $\phi_{<k}=\{\phi_{k-1},\dots\phi_{1}\}$
\end_inset

.
 This means the complexity of these distributions can explode if the number
 of lattice points 
\begin_inset Formula $N$
\end_inset

 is large, which is typically the case of interest.
 That's the reason deep neural networks have been utilized to model them
 for image/audio generation.
 However for systems with nearest neighbour interactions, the 
\emph on
dependency set
\emph default
 is significantly smaller (from my master's thesis 
\begin_inset CommandInset citation
LatexCommand cite
key "pr_2021"
literal "false"

\end_inset

).
 It's easier to show this (without loss of generality) for the nearest neighbour
 Ising model whose action is given by:
\begin_inset Formula 
\begin{equation}
S[\phi]=-\beta J\sum_{\mu=1}^{d}\sum_{x_{\nu=\mu}\in[2,L],x_{\nu\neq\mu}\in[1,L]}\phi(\bm{x}-\hat{\mu})\phi(\bm{x})\label{eq:ising_action}
\end{equation}

\end_inset

where 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

 takes values 
\begin_inset Formula $\pm1$
\end_inset

.
 Restating the autoregressive relation for the Ising model
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $k-\hat{\mu}$
\end_inset

 should be understood as the lattice position 
\begin_inset Formula $\bm{x}-\hat{\mu}$
\end_inset

 where 
\begin_inset Formula $\bm{x}$
\end_inset

 maps to 
\begin_inset Formula $k$
\end_inset

 according to the given ordering
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{equation}
\prod_{k=1}^{N}p(\phi_{k}|\phi_{<k})=p(\phi)=\exp\left(-\beta J\sum_{\mu=1}^{d}\sum_{k}\phi_{k}\phi_{k-\hat{\mu}}\right)/Z\label{eq:log_autoreg}
\end{equation}

\end_inset

In order to determine the dependency set, let's start backwards from with
 the distribution for the 
\begin_inset Formula $N$
\end_inset

th spin 
\begin_inset Formula $p(\phi_{N}|\phi_{<N})$
\end_inset

 where all other 
\begin_inset Formula $N-1$
\end_inset

 spins are known.
 We can cluster together every other term in the above equation in the form
 an unconditional probability using Bayes theorem:
\begin_inset Formula 
\[
\prod_{k=1}^{N-1}p(\phi_{k}|\phi_{<k})=p(\phi_{<N})
\]

\end_inset

so that
\begin_inset Formula 
\[
p(\phi_{N}|\phi_{<N})p(\phi_{<N})=p(\phi)
\]

\end_inset

and the unconditional probability 
\begin_inset Formula $\log p(\phi_{<N})$
\end_inset

 can be written as:
\begin_inset Formula 
\[
p(\phi_{<N})=\sum_{\phi_{N}}p(\phi)=\sum_{\phi_{N}}\exp\left(-\beta J\sum_{\mu=1}^{d}\sum_{k}\phi_{k}\phi_{k-\hat{\mu}}\right)/Z
\]

\end_inset

which leads to
\begin_inset Formula 
\[
p(\phi_{N}|\phi_{<N})=\frac{p(\phi)}{{\displaystyle \sum_{\phi_{N}}}p(\phi)}=\frac{\exp\left(-\beta J{\displaystyle \sum_{\mu=1}^{d}}{\displaystyle \sum_{k}}\phi_{k}\phi_{k-\hat{\mu}}\right)}{{\displaystyle \sum_{\phi_{N}}}\exp\left(-\beta J{\displaystyle \sum_{\mu=1}^{d}}{\displaystyle \sum_{k}}\phi_{k}\phi_{k-\hat{\mu}}\right)}
\]

\end_inset

Here, every term cancels between the numerator and denominator except the
 ones that containing 
\begin_inset Formula $\phi_{N}$
\end_inset

, since addition within exponentials beautifully factors outside:
\begin_inset Formula 
\[
p(\phi_{N}|\phi_{<N})=\frac{\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{k-\hat{\mu}}+\delta(\phi_{<N})\right)}{{\displaystyle \sum_{\phi_{N}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{k-\hat{\mu}}+\delta(\phi_{<N})\right)}=\frac{\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}\right)}{{\displaystyle \sum_{\phi_{N}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}\right)}
\]

\end_inset

So 
\begin_inset Formula $p(\phi_{N}|\phi_{<N})$
\end_inset

 doesn't depend on all the 
\begin_inset Formula $N-1$
\end_inset

 values within 
\begin_inset Formula $\phi_{<N}$
\end_inset

 but only the 
\begin_inset Formula $d$
\end_inset

 neighbouring values 
\begin_inset Formula $\phi_{N-\hat{\mu}}$
\end_inset

.
 Proceeding to the next term:
\begin_inset Formula 
\begin{align*}
p(\phi_{N-1}|\phi_{<N-1}) & =\frac{p(\phi_{<N})}{p(\phi_{<N-1})}=\frac{{\displaystyle \sum_{\phi_{N}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}+-\beta J\phi_{N-1}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-1-\hat{\mu}}+\delta(\phi_{<N-1})\right)}{{\displaystyle \sum_{\phi_{N},\phi_{N-1}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}+-\beta J\phi_{N-1}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-1-\hat{\mu}}+\delta(\phi_{<N-1})\right)}\\
 & =\frac{{\displaystyle \sum_{\phi_{N}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}+-\beta J\phi_{N-1}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-1-\hat{\mu}}\right)}{{\displaystyle \sum_{\phi_{N},\phi_{N-1}}}\exp\left(-\beta J\phi_{N}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-\hat{\mu}}+-\beta J\phi_{N-1}{\displaystyle \sum_{\mu=1}^{d}}\phi_{N-1-\hat{\mu}}\right)}
\end{align*}

\end_inset

and this depends on the neighbours of both 
\begin_inset Formula $\phi_{N}$
\end_inset

 and 
\begin_inset Formula $\phi_{N-1}$
\end_inset

.
 In general, we have for 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

:
\begin_inset Formula 
\begin{align}
p(\phi_{k}|\phi_{<k}) & =\frac{{\displaystyle \sum_{\phi_{N},\dots\phi_{k+1}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}{{\displaystyle \sum_{\phi_{N},\dots\phi_{k}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}\label{eq:ising_k_conditional}
\end{align}

\end_inset

and the dependency set here contains the values of neighbours of 
\begin_inset Formula $\phi_{>k}$
\end_inset

 contained within 
\begin_inset Formula $\phi_{<k}$
\end_inset

.
 We can draw the same conclusion for scalar lattice field theory by replacing
 the sums with integrals and including terms like 
\begin_inset Formula $\phi_{l}^{2}$
\end_inset

 and 
\begin_inset Formula $\phi_{l}^{4}$
\end_inset

 in the above expression.
 The number of elements in the dependency set is bounded above by 
\begin_inset Formula $L^{d-1}$
\end_inset

 or 
\begin_inset Formula $N/L$
\end_inset

 (due to geometric constraints, see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2D-dep-field"

\end_inset

 for an illustration on a 2D lattice) which is an 
\begin_inset Quotes eld
\end_inset

order of magnitude
\begin_inset Quotes erd
\end_inset

 smaller than the original upper bound 
\begin_inset Formula $N$
\end_inset

.
 In fact, we can join 2 strips of black spins in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2D-dep-field"

\end_inset

 into a single 1D line, and the conditional distribution on 
\begin_inset Formula $\phi_{k}$
\end_inset

 simply depends on the values along this line.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/boltz_dep_field.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
In the 2 dimensional 
\begin_inset Formula $10\times10$
\end_inset

 lattice above, the conditional probability 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 of the red spin depends only on the nearest neighbours of the spins in
 
\begin_inset Formula $\phi_{>k}$
\end_inset

 (coloured white), within 
\begin_inset Formula $\phi_{<k}$
\end_inset

.
 Hence the dependency set is only the 
\begin_inset Formula $L=10$
\end_inset

 spins coloured black and doesn't contain the grey ones above it.
\begin_inset CommandInset label
LatexCommand label
name "fig:2D-dep-field"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It takes a bit more imagination to convince oneself that this can be generalized
 for higher dimensional lattices.
 For 
\begin_inset Formula $d$
\end_inset

 dimensional lattices, 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 depends on a 
\begin_inset Formula $d-1$
\end_inset

 dimensional box that can parametrically constructed using:
\begin_inset Formula 
\begin{align*}
B_{\bm{x}}(y_{1},\dots y_{d-1}) & =\begin{cases}
[y_{1},\dots,y_{d-1},x_{d}] & \text{if }k(y_{1},\dots,y_{d-1},x_{d})<k(\bm{x})\\{}
[y_{1},\dots,y_{d-1},x_{d}-1] & \text{if }k(y_{1},\dots,y_{d-1},x_{d})>k(\bm{x})
\end{cases}
\end{align*}

\end_inset

What if we have second nearest neighbour interactions? The dependency set
 would be simply expanded to another 
\begin_inset Formula $d-1$
\end_inset

 dimensional box above the 
\begin_inset Formula $B_{x}$
\end_inset

 for nearest neighbour.
 A 
\emph on
local
\emph default
 action principle (or nothing spooky at a distance) is a fundamental property
 of literally anything in physics so this mathematical result should be
 equally useful for more complex lattice field theories as well.
\end_layout

\begin_layout Section
Neural network ansatz for autoregressive sampling
\end_layout

\begin_layout Standard
We can model the distribution 
\begin_inset Formula $p(\phi_{k}|B_{k})$
\end_inset

 using a neural network ansatz and sample lattice values sequentially, similar
 to MADE or PixelCNN.
 For example, we can let the outputs of the 
\begin_inset Formula $k^{th}$
\end_inset

 neural network parameterize a mixture of 
\begin_inset Formula $M$
\end_inset

 Gaussians:
\begin_inset Formula 
\begin{align*}
\left\{ w_{j},\mu_{j},\sigma_{j}\right\} _{j=1}^{M} & =NN_{k}(B_{k})\\
p(\phi_{k}|B_{k}) & \approx\sum_{j}w_{j}\mathcal{N}(\mu_{j},\sigma_{j})
\end{align*}

\end_inset

which is flexible, as well as easy to sample from.
 We can exploit the translational invariance of the system, drop the 
\begin_inset Formula $k$
\end_inset

 subscript and sample using the same neural network 
\begin_inset Formula $NN$
\end_inset

 at every position- an approximation that gets better as 
\begin_inset Formula $L$
\end_inset

 gets large.
 This ensures the number of neural network weights do not scale with system
 size and also enables a scalable model where a network trained on smaller
 
\begin_inset Formula $L$
\end_inset

 can be reused to sample a larger lattice- which should be crucial for lattice
 field theories where the cost of simulating systems typically scale with
 the system size.
 Since the translational invariance is only approximate, we should expect
 errors to increase when we do an extrapolation to large 
\begin_inset Formula $L$
\end_inset

.
 It will be interesting to see if we can engineer the model architecture
 or the input data to address such sources of error.
 The log likelihood at every position can be accumulated and optimized using
 the REINFORCE estimator of the KL divergence between the ansatz and the
 unnormalized Boltzmann distribution (see 
\begin_inset CommandInset citation
LatexCommand cite
key "wu2021unbiased"
literal "false"

\end_inset

 for a treatment of the Ising model).
\end_layout

\begin_layout Standard
An intriguing alternative would be to model the 2-variable conditional distribut
ion 
\begin_inset Formula $p(\phi_{k+1},\phi_{k}|B_{k})$
\end_inset

 using a flow-based network like RealNVP
\begin_inset CommandInset citation
LatexCommand cite
key "dinh2016density"
literal "false"

\end_inset

.
 It uses a much more flexible ansatz compared to mixture of Gaussians and
 
\emph on
reparameterizable
\emph default
 sampling of the conditionals allows us to optimize the KL divergence directly,
 and mitigates issues like variance when using the REINFORCE estimator.
 This would essentially be a compact and scalable version of 
\begin_inset CommandInset citation
LatexCommand cite
key "albergo2019flow"
literal "false"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This is an oversimplified picture and there are differences like periodic
 vs open boundary conditions.
 Assumption of translational invariance on a finite lattice can contribute
 to errors which can require more careful model construction to address.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
While approaches using autoregressive networks for sampling lattice field
 theories already exist 
\begin_inset CommandInset citation
LatexCommand cite
key "luo2021gauge"
literal "false"

\end_inset

, expoiting the 
\begin_inset Formula $d-1$
\end_inset

 dimensional dependency set means the neural network 
\begin_inset Formula $NN$
\end_inset

 can be a 
\begin_inset Formula $d-1$
\end_inset

 dimensional convolutional network- which can be designed to model stronger
 dependence on positions closer to 
\begin_inset Formula $\phi_{k}$
\end_inset

 than farther ones.
 A fascinating outcome of lower dimensional inputs is that in the practically
 important case of 
\begin_inset Formula $d=4$
\end_inset

, it's sufficient to use 3D convolutional layers that have optimized GPU
 implementations in the CUDA stack or popular deep learning frameworks like
 PyTorch or Tensorflow- the same are typically absent for 4D convolutions.
 In a nutshell (and best of my perception), the compact autoregressive network
 proposed here would be to 
\begin_inset Quotes eld
\end_inset

local action principle
\begin_inset Quotes erd
\end_inset

 what convolutional neural networks is to 
\begin_inset Quotes eld
\end_inset

translationally invariant data
\begin_inset Quotes erd
\end_inset

.
 In more general terms, lower-sized inputs helped tackle the curse of dimensiona
lity in gradient-based models and persuits for them have yielded successful
 architectures in the Geometric Deep Learning framework
\begin_inset CommandInset citation
LatexCommand cite
key "bronstein2017geometric"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref"
options "biblatex"

\end_inset


\end_layout

\end_body
\end_document
