#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine biblatex
\cite_engine_type authoryear
\biblatex_bibstyle numeric
\biblatex_citestyle numeric
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Locality constrained autoregressive networks for lattice field theory simulation
s
\end_layout

\begin_layout Section
The autoregressive relation and scalar lattice field theory
\end_layout

\begin_layout Standard
The systems of interest consist of a box lattice of length 
\begin_inset Formula $L$
\end_inset

 in 
\begin_inset Formula $d$
\end_inset

 dimensions where every position is labeled using a 
\begin_inset Formula $d$
\end_inset

-dimensional vector 
\begin_inset Formula $\bm{x}\in[1,L]^{d}$
\end_inset

.
 The system state/configuration is described using scalar values at every
 position 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

.
 The configurations obey the Boltzmann distribution:
\begin_inset Formula 
\begin{equation}
p\left(\{\phi(\bm{x})\}_{\bm{x}\in[1,L]^{d}}\right)=e^{-S[\phi]}/Z\label{eq:boltzmann}
\end{equation}

\end_inset

where the 
\emph on
action
\emph default
 
\begin_inset Formula $S[\phi]$
\end_inset

 is a functional of the field values 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

.
 The d-dimensional positions 
\begin_inset Formula $\bm{x}$
\end_inset

 maybe replaced with a 1 dimensional ordering:
\begin_inset Formula 
\begin{equation}
k=\left({\displaystyle \sum_{i=1}^{d}}(x_{i}-1)L^{i-1}\right)+1\label{eq:ordering}
\end{equation}

\end_inset

where 
\begin_inset Formula $x_{i}$
\end_inset

 are the components of 
\begin_inset Formula $\bm{x}$
\end_inset

 and 
\begin_inset Formula $k\in[1,N=L^{d}]$
\end_inset

.
 Based on this ordering, we can write down the probability distribution
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:boltzmann"

\end_inset

 as a product of conditional distributions at every position:
\begin_inset Formula 
\begin{align}
p(\{\phi_{k}\}) & =p(\phi_{1},\phi_{2}\dots\phi_{N})=p(\phi_{1})p(\phi_{2}|\phi_{1})\dots p(\phi_{N}|\phi_{N-1}\dots\phi_{2},\phi_{1})\nonumber \\
 & =\prod_{k\in[1,N]}p(\phi_{k}|\phi_{<k})\label{eq:chainrule}
\end{align}

\end_inset

This is the chain rule of conditional probabilities based on Bayes theorem
 or 
\emph on
autoregressive relation
\emph default
.
 This mathematical relation is the basis of image and audio generation algorithm
s in deep learning such as MADE
\begin_inset CommandInset citation
LatexCommand cite
key "made"
literal "false"

\end_inset

 and PixelCNN
\begin_inset CommandInset citation
LatexCommand cite
key "pixelcnn"
literal "false"

\end_inset

.
 Our system of interest is the scalar lattice field theory whose action
 is given by:
\begin_inset Formula 
\begin{equation}
S[\phi]={\displaystyle \sum_{\bm{x}\in[1,L]^{d}}}\left[\phi(\bm{x}){\displaystyle \sum_{\bm{y}}}\boxempty(\bm{x},\bm{y})\phi(\bm{y})+m^{2}\phi(\bm{x})^{2}+\lambda\phi(\bm{x})^{4}\right]\label{eq:latticeFT-action}
\end{equation}

\end_inset

where 
\begin_inset Formula $a$
\end_inset

, 
\begin_inset Formula $m$
\end_inset

, 
\begin_inset Formula $\lambda$
\end_inset

 are the lattice spacing, mass and coupling respectively.
 Assuming open boundary conditions, we can expand the d'Alembertian term
 in the RHS as:
\begin_inset Formula 
\[
{\displaystyle \sum_{\bm{x}\in[1,L]^{d}}}\phi(\bm{x}){\displaystyle \sum_{\bm{y}}}\boxempty(\bm{x},\bm{y})\phi(\bm{y})=\sum_{\mu=1}^{d}\sum_{x_{\nu=\mu}\in[2,L-1],x_{\nu}\neq[1,L]}2\phi(\bm{x})^{2}-\phi(\bm{x})\phi(\bm{x}-\hat{\mu})-\phi(\bm{x})\phi(\bm{x}+\hat{\mu})
\]

\end_inset

and 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

 can take any real value.
 Note that 
\begin_inset Formula $S[\phi]$
\end_inset

 contains only nearest neighbour product/interaction terms 
\begin_inset Formula $\phi(\bm{x})\phi(\bm{x}-\hat{\mu})$
\end_inset

 and 
\begin_inset Formula $\phi(\bm{x})\phi(\bm{x}+\hat{\mu})$
\end_inset

, other than powers of 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

.
 This property of the action is known as 
\emph on
locality
\emph default
 which is obeyed by more complex lattice field theory systems as well
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
In more famous words, 
\begin_inset Quotes eld
\end_inset

there's no spooky action at a distance
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_inset

.
\end_layout

\begin_layout Section
Smaller dependency sets of conditional distributions due to nearest neighbour
 interactions
\end_layout

\begin_layout Standard
Examining the 
\begin_inset Formula $k$
\end_inset

th conditional probability 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:chainrule"

\end_inset

, its distribution in general depends on 
\begin_inset Formula $k-1$
\end_inset

 values in 
\begin_inset Formula $\phi_{<k}=\{\phi_{k-1},\dots\phi_{1}\}$
\end_inset

.
 This means the complexity of these distributions can explode if the number
 of lattice points 
\begin_inset Formula $N$
\end_inset

 is large, which is typically the case of interest.
 That's the reason deep neural networks have been utilized to model them
 for image/audio generation.
 However for systems with nearest neighbour interactions, the 
\emph on
dependency set
\emph default
 is significantly smaller (from my master's thesis 
\begin_inset CommandInset citation
LatexCommand cite
key "pr_2021"
literal "false"

\end_inset

).
 It's easier to show this (without loss of generality) for the nearest neighbour
 Ising model whose action is given by:
\begin_inset Formula 
\begin{equation}
S[\phi]=-\beta J\sum_{\mu=1}^{d}\sum_{x_{\nu=\mu}\in[2,L],x_{\nu\neq\mu}\in[1,L]}\phi(\bm{x}-\hat{\mu})\phi(\bm{x})\label{eq:ising_action}
\end{equation}

\end_inset

where 
\begin_inset Formula $\phi(\bm{x})$
\end_inset

 takes values 
\begin_inset Formula $\pm1$
\end_inset

.
 Restating the autoregressive relation for the Ising model
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
\begin_inset Formula $k-\hat{\mu}$
\end_inset

 should be understood as the lattice position 
\begin_inset Formula $\bm{x}-\hat{\mu}$
\end_inset

 where 
\begin_inset Formula $\bm{x}$
\end_inset

 maps to 
\begin_inset Formula $k$
\end_inset

 according to the given ordering
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{equation}
\prod_{k=1}^{N}p(\phi_{k}|\phi_{<k})=p(\phi)=\exp\left(-\beta J\sum_{\mu=1}^{d}\sum_{k}\phi_{k}\phi_{k-\hat{\mu}}\right)/Z\label{eq:log_autoreg}
\end{equation}

\end_inset

From Bayes theorem, we can relate this conditional probability to the unconditio
nal probabilities of the first 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $k-1$
\end_inset

 spins, which can in turn be written as reduced forms of the Boltzmann distribut
ion:
\begin_inset Formula 
\[
p(\phi_{k}|\phi_{<k})=\frac{p(\phi_{1},\dots\phi_{k})}{p(\phi_{1},\dots\phi_{k-1})}=\frac{{\displaystyle \sum_{\phi_{k+1}\dots\phi_{N}}}p(\phi)}{{\displaystyle \sum_{\phi_{k}\dots\phi_{N}}}p(\phi)}
\]

\end_inset

Expanding the 
\begin_inset Formula $p(\phi)$
\end_inset

 for the Ising model:
\begin_inset Formula 
\begin{align*}
p(\phi_{k}|\phi_{<k}) & =\frac{{\displaystyle \sum_{\phi_{N},\dots\phi_{k+1}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)+\delta(\phi_{<k})\right)}{{\displaystyle \sum_{\phi_{N},\dots\phi_{k}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)+\delta(\phi_{<k})\right)}
\end{align*}

\end_inset

Since the values in 
\begin_inset Formula $\phi_{<k}$
\end_inset

 are fixed and not summed over, the terms 
\begin_inset Formula $\delta(\phi_{<k})$
\end_inset

 containing only them cancel from the numerator and denominator, leaving
 us with:
\begin_inset Formula 
\begin{align}
p(\phi_{k}|\phi_{<k}) & =\frac{{\displaystyle \sum_{\phi_{N},\dots\phi_{k+1}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}{{\displaystyle \sum_{\phi_{N},\dots\phi_{k}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}\label{eq:ising_k_conditional}
\end{align}

\end_inset

Even though 
\begin_inset Formula $\phi_{<k}$
\end_inset

 contains 
\begin_inset Formula $k-1$
\end_inset

 values, the conditional probability 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 depends only on those positions within 
\begin_inset Formula $\phi_{<k}$
\end_inset

 that are neighbours of the positions in 
\begin_inset Formula $\phi_{\geq k}$
\end_inset

.
 We can draw the same conclusion for scalar lattice field theory by replacing
 the sums with integrals and including terms like 
\begin_inset Formula $\phi_{l}^{2}$
\end_inset

 and 
\begin_inset Formula $\phi_{l}^{4}$
\end_inset

 in the above expression.
 The number of elements in the dependency set is bounded above by 
\begin_inset Formula $L^{d-1}$
\end_inset

 or 
\begin_inset Formula $N/L$
\end_inset

 for our choice of ordering (see figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2D-dep-field"

\end_inset

 for an illustration on a 2D lattice) which is an 
\begin_inset Quotes eld
\end_inset

order of magnitude
\begin_inset Quotes erd
\end_inset

 smaller than the original upper bound 
\begin_inset Formula $N$
\end_inset

.
 In fact, we can join 2 strips of black spins in figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:2D-dep-field"

\end_inset

 into a single 1D line, and the conditional distribution on 
\begin_inset Formula $\phi_{k}$
\end_inset

 simply depends on the values along this line.
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Figures/boltz_dep_field.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
In the 2 dimensional 
\begin_inset Formula $10\times10$
\end_inset

 lattice above, the conditional probability 
\begin_inset Formula $p(\phi_{k}|\phi_{<k})$
\end_inset

 of the red spin depends only on the nearest neighbours of the spins in
 
\begin_inset Formula $\phi_{>k}$
\end_inset

 (coloured white), within 
\begin_inset Formula $\phi_{<k}$
\end_inset

.
 Hence the dependency set is only the 
\begin_inset Formula $L=10$
\end_inset

 spins coloured black and doesn't contain the grey ones above it.
\begin_inset CommandInset label
LatexCommand label
name "fig:2D-dep-field"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Dependency surfaces for single and joint conditional distributions
\end_layout

\begin_layout Standard
We can also mark the dependency set of 
\begin_inset Formula $\phi_{k(\bm{x})}$
\end_inset

 as a 
\begin_inset Formula $d-1$
\end_inset

 dimensional surface constructed parametrically using
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
The reader is urged to spend some time grasping this, perhaps with aid from
 figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:dep-set"

\end_inset

 for the case of 
\begin_inset Formula $d=2$
\end_inset

.
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{equation}
B_{\bm{x}}(y_{1},\dots y_{d-1})=\begin{cases}
[y_{1},\dots,y_{d-1},x_{d}] & \text{if }k(y_{1},\dots,y_{d-1},x_{d})<k(\bm{x})\\{}
[y_{1},\dots,y_{d-1},x_{d}-1] & \text{if }k(y_{1},\dots,y_{d-1},x_{d})>k(\bm{x})
\end{cases}\label{eq:dep_box}
\end{equation}

\end_inset

We'll call this the 
\emph on
dependency surface
\emph default
 at 
\begin_inset Formula $\bm{x}$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:ising_k_conditional"

\end_inset

, we can write down the joint conditional probabilities for more than one
 variable.
 For example, we can write:
\begin_inset Formula 
\begin{align}
p(\phi_{k},\phi_{k+1}|\phi_{<k}) & =p(\phi_{k+1}|\phi_{<k+1})p(\phi_{k}|\phi_{<k})\nonumber \\
 & =\frac{{\displaystyle \sum_{\phi_{N},\dots\phi_{k+2}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}{{\displaystyle \sum_{\phi_{N},\dots\phi_{k}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}\label{eq:2-conditional}
\end{align}

\end_inset

Here, if the 
\begin_inset Formula $x_{d}$
\end_inset

 is the same for both 
\begin_inset Formula $k$
\end_inset

 and 
\begin_inset Formula $k+1$
\end_inset

, then the dependency surface for this joint conditional porbability is
 still the same 
\begin_inset Formula $B_{\bm{x}}$
\end_inset

! For a particular choice of 
\begin_inset Formula $\bm{x}=[1,\dots,1,t]$
\end_inset

, the dependency surface 
\begin_inset Formula $B_{\bm{x}}$
\end_inset

 =
\begin_inset Formula $[y_{1},\dots,y_{d-1},t-1]$
\end_inset

 since the case 1 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:dep_box"

\end_inset

 never arises.
 By extension of the logic in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2-conditional"

\end_inset

, the dependency of the surface 
\begin_inset Formula $x_{d}=t$
\end_inset

 can be inferred from:
\begin_inset Formula 
\begin{equation}
p(\phi_{k(\bm{x})},\dots\phi_{k(\bm{x})+L^{d-1}-1}|\phi_{<k})=\frac{{\displaystyle \sum_{\phi_{N},\dots\phi_{k+L^{d-1}-1}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}{{\displaystyle \sum_{\phi_{N},\dots\phi_{k}}}\exp\left(-\beta J{\displaystyle \sum_{l=k}^{N}}\left(\phi_{l}{\displaystyle \sum_{\mu}}\phi_{l-\hat{\mu}}\right)\right)}\label{eq:surface-conditional}
\end{equation}

\end_inset

and that's simply 
\begin_inset Formula $B_{\bm{x}}$
\end_inset

.
 In simpler terms, the joint probability of the surface 
\begin_inset Formula $x_{d}=t$
\end_inset

 is conditioned only on the surface 
\begin_inset Formula $x_{d}=t-1$
\end_inset

: it's similar to propagating a stochastic differential equation from an
 initial value!
\end_layout

\begin_layout Section
Neural network ansatz for autoregressive sampling
\end_layout

\begin_layout Standard
We can model the distribution 
\begin_inset Formula $p(\phi_{k}|B_{k})$
\end_inset

 using a neural network ansatz and sample lattice values sequentially, similar
 to MADE or PixelCNN.
 For example, we can let the outputs of the 
\begin_inset Formula $k^{th}$
\end_inset

 neural network parameterize a mixture of 
\begin_inset Formula $M$
\end_inset

 Gaussians:
\begin_inset Formula 
\begin{align*}
\left\{ w_{j},\mu_{j},\sigma_{j}\right\} _{j=1}^{M} & =NN_{k}(B_{k})\\
p(\phi_{k}|B_{k}) & \approx\sum_{j}w_{j}\mathcal{N}(\mu_{j},\sigma_{j})
\end{align*}

\end_inset

which is flexible, as well as easy to sample from.
 We can exploit the translational invariance of the system, drop the 
\begin_inset Formula $k$
\end_inset

 subscript and sample using the same neural network 
\begin_inset Formula $NN$
\end_inset

 at every position- an approximation that gets better as 
\begin_inset Formula $L$
\end_inset

 gets large.
 This ensures the number of neural network weights do not scale with system
 size and also enables a scalable model where a network trained on smaller
 
\begin_inset Formula $L$
\end_inset

 can be reused to sample a larger lattice- which should be crucial for lattice
 field theories where the cost of simulating systems typically scale with
 the system size.
 Since the translational invariance is only approximate, we should expect
 errors to increase when we do an extrapolation to large 
\begin_inset Formula $L$
\end_inset

.
 It will be interesting to see if we can engineer the model architecture
 or the input data to address such sources of error.
 The log likelihood at every position can be accumulated and optimized using
 the REINFORCE estimator of the KL divergence between the ansatz and the
 unnormalized Boltzmann distribution (see 
\begin_inset CommandInset citation
LatexCommand cite
key "wu2021unbiased"
literal "false"

\end_inset

 for a treatment of the Ising model).
\end_layout

\begin_layout Standard
We can model the 2-variable conditional distribution 
\begin_inset Formula $p(\phi_{k+1},\phi_{k}|B_{k})$
\end_inset

 from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:2-conditional"

\end_inset

 using a flow-based network like RealNVP
\begin_inset CommandInset citation
LatexCommand cite
key "dinh2016density"
literal "false"

\end_inset

 where the prior distribution would be Gaussians parameterized by a convolutiona
l network acting on 
\begin_inset Formula $B_{k}$
\end_inset

.
 It uses a much more flexible ansatz compared to mixture of Gaussians and
 
\emph on
reparameterizable
\emph default
 sampling of the conditionals allows us to optimize the KL divergence directly,
 and mitigates issues like variance when using the REINFORCE estimator.
 This can essentially be a compact (and scalable if it's the 2 variable
 model) versions of a model that uses a flow-based network to sample the
 entire lattice like in 
\begin_inset CommandInset citation
LatexCommand cite
key "albergo2019flow"
literal "false"

\end_inset


\begin_inset Foot
status collapsed

\begin_layout Plain Layout
This is an oversimplified picture and there are differences like periodic
 vs open boundary conditions.
 Assumption of translational invariance on a finite lattice can contribute
 to errors which can require more careful model construction to address.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
We can also model the quantity 
\begin_inset Formula $p\left(\{\phi_{\bm{x}}\}_{x_{d}=t}|\{\phi_{\bm{x}}\}_{x_{d}=t-1}\right)$
\end_inset

 from 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:surface-conditional"

\end_inset

 whose dependency set would be the surface 
\begin_inset Formula $x_{d}=t-1$
\end_inset

, using flow based networks.
 Since this samples one time step at a time, this model can be considered
 an unsupervised version of neural SDEs.
\end_layout

\begin_layout Standard
While approaches using autoregressive networks for sampling lattice field
 theories already exist 
\begin_inset CommandInset citation
LatexCommand cite
key "luo2021gauge"
literal "false"

\end_inset

, expoiting the 
\begin_inset Formula $d-1$
\end_inset

 dimensional dependency set means the neural network 
\begin_inset Formula $NN$
\end_inset

 can be a 
\begin_inset Formula $d-1$
\end_inset

 dimensional convolutional network- which can be designed to model stronger
 dependence on positions closer to 
\begin_inset Formula $\phi_{k}$
\end_inset

 than farther ones.
 A fascinating outcome of lower dimensional inputs is that in the practically
 important case of 
\begin_inset Formula $d=4$
\end_inset

, it's sufficient to use 3D convolutional layers that have optimized GPU
 implementations in the CUDA stack or popular deep learning frameworks like
 PyTorch or Tensorflow- the same are typically absent for 4D convolutions.
 More generally, smaller input space of the proposed class of models help
 tackle the curse of dimensionality that usually plagues neural networks-
 and can be a 
\begin_inset Quotes eld
\end_inset

locality
\begin_inset Quotes erd
\end_inset

 addition to the Geometric Deep Learning framework
\begin_inset CommandInset citation
LatexCommand cite
key "bronstein2017geometric"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "ref"
options "biblatex"

\end_inset


\end_layout

\end_body
\end_document
